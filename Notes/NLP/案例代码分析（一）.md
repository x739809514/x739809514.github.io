## Glove Intro

```python
import torch
import torchtext.vocab as vocab
```
导入库，torch是pytorch, 他是一个深度学习框架，torchtext是pytorch的一个子模块

```python
from torchtext.vocab import Vectors

class newGloVe(Vectors):

    url = {
        "42B": "https://huggingface.co/stanfordnlp/glove/resolve/main/glove.42B.300d.zip",
        "840B": "https://huggingface.co/stanfordnlp/glove/resolve/main/glove.840B.300d.zip",
        "twitter.27B": "https://huggingface.co/stanfordnlp/glove/resolve/main/glove.twitter.27B.zip",
        "6B": "https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip",
    }
    
    def __init__(self, name="840B", dim=300, **kwargs) -> None:
        url = self.url[name]
        print(f"Downloading from {url}")
        name = "glove.{}.{}d.txt".format(name, str(dim))
        super(newGloVe, self).__init__(name, url=url, **kwargs)
        
glove = newGloVe(name='6B', dim=100)
```
这里加载对应版本的glove，这里采用6B的模型，维度为100

```python
glove.vectors.shape
```
`glove.vectors` 是一个二维的张量，其中每一行表示一个单词的词嵌入向量。 `.shape` 属性返回一个包含两个值的元组，第一个值是词汇表中的单词数量，第二个值是每个词嵌入向量的维度。

```python
def get_embedding_vector(word):
    word_index = glove.stoi[word]
    emb = glove.vectors[word_index]
    return emb

get_embedding_vector('chess').shape
```
这里将传入的word映射到向量空间，也就是词嵌入（Word Embedding）

```python
def get_closest_words_from_word(word, max_n=5):
    word_emb = get_embedding_vector(word)
    distances = [(w, torch.dist(word_emb, get_embedding_vector(w)).cpu().item()) for w in glove.itos]
    dist_sort_filt = sorted(distances, key=lambda x: x[1])[:max_n]
    return dist_sort_filt

get_closest_words_from_word('chess')
```
这里根据获得的word embedding 计算余弦相似度相近的前五个word, 余弦相似度越趋向于1，说明语义越相近

```python
def get_closest_words_from_embedding(word_emb, max_n=5):
    distances = [(w, torch.dist(word_emb, get_embedding_vector(w)).cpu().item()) for w in glove.itos]
    dist_sort_filt = sorted(distances, key=lambda x: x[1])[:max_n]
    return dist_sort_filt
    
def get_word_analogy(word1, word2, word3, max_n=5):
    # logic w1= king, ...
    # w2 - w1 + w3 --> w4
    word1_emb = get_embedding_vector(word1)
    word2_emb = get_embedding_vector(word2)
    word3_emb = get_embedding_vector(word3)
    word4_emb = word2_emb - word1_emb + word3_emb
    analogy = get_closest_words_from_embedding(word4_emb)
    return analogy

get_word_analogy(word1='sister', word2='brother', word3='nephew')
```
这里在做词分析，但是逻辑跟上面的是一样的，先计算word映射出来的向量，再计算向量的余弦相似度，并根据结果出于word4_emb最相近的一个向量，在转化成word，这样就能预测出我要的与sister，brother关系相近的并基于先决条件nephew的另一个单词了
## Word Cluster

核心逻辑是通过词嵌入技术先将输入词转化到向量空间当中，然后找到在向量空间中与之距离相近的向量，然后再转化为word, 核心还是还是word embedding 的应用。

代码解析：
```python
def get_embedding_vector(word):
    word_index = glove.stoi[word]
    emb = glove.vectors[word_index]
    return emb

def get_closest_words_from_word(word, max_n=5):
    word_emb = get_embedding_vector(word)
    distances = [(w, torch.dist(word_emb, get_embedding_vector(w)).cpu().item()) for w in glove.itos]
    dist_sort_filt = sorted(distances, key=lambda x: x[1])[:max_n]
    return [item[0] for item in dist_sort_filt]

get_closest_words_from_word(word='chess', max_n=10)
```
可以看到上面的两个函数是一个是使用glove模型实行word embedding. 第二个函数是根据获得的word_emb来获得向量空间中距离相信的word

```python
words = []
categories = ['numbers', 'algebra', 'music', 'science', 'technology']


df_word_cloud = pd.DataFrame({
    'category': [],
    'word': []
})

for category in categories:
    print(category)
    closest_words = get_closest_words_from_word(word=category, max_n=20)
    temp = pd.DataFrame({
        'category': [category] * len(closest_words),
        'word': closest_words
    })

    df_word_cloud = pd.concat([df_word_cloud, temp], ignore_index=True)
```
这段代码其实就是在具体做cluster，逻辑是先定义需要实现cluster的类别，然后根据类别执行上面的function，最后把结果放到数据帧里去

```python
n_rows = df_word_cloud.shape[0]
n_cols = glove_dim # here glove_dim is 100
X = torch.empty((n_rows, n_cols))

for i in range(n_rows):
    current_word = df_word_cloud.loc[i, 'word']
    X[i, :] = get_embedding_vector(current_word)
    print(f"{i}: {current_word}")
```
上面这段函数是让聚类中的每一个word的向量有100个维度， 最后打印出来

```python
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X.cpu().numpy())

# %%
df_word_cloud['x'] = X_tsne[:, 0]
df_word_cloud['y'] = X_tsne[:, 1]

ggplot(data=df_word_cloud.sample(25)) + aes(x = 'x', y='y', label = 'word', color = 'category') + geom_text() + labs(title='GloVe Word Embeddings and Categories')
```
上面这段代码的前半部分是在做数据降维， 使用t-sne方法，把维度降到2
后面是在画cluster的图
## Sentiment Analysis

```
Text Corpus -> Pre-trained Word Embedding model -> Word Embedding -> Nerual Network
```
情感分析是一个词嵌入一个很好的应用，它使用词嵌入或者文本嵌入表示文本，然后用分类器表达情感极性，下面这个例子使用的是sentence embedding.

代码分析：

```python
import numpy as np
import pandas as pd
import pickle
from collections import Counter
import seaborn as sns
  
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
# sentense embedding model
from sentence_transformers import SentenceTransformer
```
导入依赖，不必多说

```python
twitter_file = 'data/Tweets.csv'
	df = pd.read_csv(twitter_file).dropna()
df
#%% Create Target Variable
cat_id = {'neutral': 1,
          'negative': 0,
          'positive': 2}

df['class'] = df['sentiment'].map(cat_id)
```
这里首先引入数据，然后定义情感极性，并在数据帧最后添加一列表示情感极性

```python
BATCH_SIZE = 128
NUM_EPOCHS = 80
MAX_FEATURES = 10
```
定义超参

```python
#%% Embedding Model
emb_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v1')
sentences = [ "Each sentence is converted"]
embeddings = emb_model.encode(sentences)
print(embeddings.squeeze().shape)
```
这里在做句嵌入，并用一个sentence做个一个小测验

```python
#%% prepare X and y
# X = emb_model.encode(df['text'].values)

# with open("data/tweets_X.pkl", "wb") as output_file:
#     pickle.dump(X, output_file)
with open("data/tweets_X.pkl", "rb") as input_file:
    X = pickle.load(input_file)

y = df['class'].values
```
准备用于训练的X和y, 这里的X是把所有text列的sentence全部使用句嵌入，y则是期待的情感极性输出，这里是class列，上面的代码中新增的

```python
#%% Train Val Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)

# %%
class SentimentData(Dataset):
    def __init__(self, X, y):
        super().__init__()
        self.X = torch.Tensor(X)
        self.y = torch.Tensor(y).type(torch.LongTensor)
        self.len = len(self.X)
# 获取数据集长度 __len__
    def __len__(self):
        return self.len
# 获取指定索引的数据 __getitem__
    def __getitem__(self, index):
        return self.X[index], self.y[index]

train_ds = SentimentData(X= X_train, y = y_train)
test_ds = SentimentData(X_test, y_test)
```
上面的代码第一步对数据集进行了分割，第二步形成训练的数据结构，它继承自PyTorch中的dataset类，将X，y转化成张量，其中y需要时长整型，最后使用这个类创建训练数据集和测试数据集

```python
# %% Dataloader
train_loader = DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_ds, batch_size=15000)

# %% Model
class SentimentModel(nn.Module):
    def __init__(self, NUM_FEATURES, NUM_CLASSES, HIDDEN = 10):
        super().__init__()
        self.linear = nn.Linear(NUM_FEATURES, HIDDEN) # 全连接层1
        self.linear2 = nn.Linear(HIDDEN, NUM_CLASSES) # 全连接层2
        self.relu = nn.ReLU() # 激活函数
        self.log_softmax = nn.LogSoftmax(dim=1) # 输出层激活函数

    def forward(self, x):
        x = self.linear(x)
        x = self.relu(x)
        x = self.linear2(x)
        x = self.log_softmax(x)
        return x
```
显示加载数据，后面的代码是一个简单的神经网络模型，有两个全连接层，一个隐藏层

```python
model = SentimentModel(NUM_FEATURES = X_train.shape[1], NUM_CLASSES = 3)
criterion = nn.CrossEntropyLoss() # 损失函数
optimizer = torch.optim.AdamW(model.parameters())
```
定义模型，损失函数和优化器

```python
# %% Model Training
train_losses = []

for e in range(NUM_EPOCHS):
    curr_loss = 0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad() # clear gard
        y_pred_log = model(X_batch) # forward difussion
        loss = criterion(y_pred_log, y_batch.long())
        curr_loss += loss.item() # cal loss
        loss.backward() # inverse
        optimizer.step() # update weight
    train_losses.append(curr_loss)

    print(f"Epoch {e}, Loss: {curr_loss}")

sns.lineplot(x=list(range(len(train_losses))), y= train_losses)
```
这里就是在训练了， 里面涉及到梯度清理，前向，反向传播，梯度下降等，训练的目的就是更新权重，也就是参数，这个是通过梯度下降来实现的，最后将每一步的损失打印出来，还画出来损失曲线。

```python
# 禁用梯度，节省性能
with torch.no_grad():
    for X_batch, y_batch in test_loader:
        y_test_pred_log = model(X_batch)
        y_test_pred = torch.argmax(y_test_pred_log, dim = 1)

# 预测结果转成numpy数组
y_test_pred_np = y_test_pred.squeeze().cpu().numpy()

# 评估准确性
acc = accuracy_score(y_pred=y_test_pred_np, y_true = y_test)
f"The accuracy of the model is {np.round(acc, 3)*100}%."

# 评估朴素分类器的准确性
most_common_cnt = Counter(y_test).most_common()[0][1]
print(f"Naive Classifier: {np.round(most_common_cnt / len(y_test) * 100, 1)} %")
```

这一部分就是在评估模型了。其中朴素分类器是评估模型的一项很重要的指标
### 朴素分类器的定义和作用

朴素分类器是一种简单的基线分类方法，它总是预测数据集中最常见的类别。例如，如果在一个二分类问题中，90%的样本属于类别A，朴素分类器会总是预测所有样本都属于类别A。这个分类器虽然简单，但在不平衡数据集（某些类别比其他类别出现频率高得多）中，它的准确性可能会相当高。

### 引入朴素分类器评估的目的

1. **建立基线**：
    
    - 朴素分类器提供了一个简单的基线，供模型性能进行对比。如果模型的准确性无法明显超过朴素分类器，那么模型可能没有捕捉到数据中的有效模式，或者模型过于复杂，无法超越简单的预测策略。
2. **衡量模型的有效性**：
    
    - 通过与朴素分类器的比较，可以评估模型是否真正有效。高于朴素分类器的准确性表明模型确实从数据中学到了有用的信息，而不是仅仅依赖于数据集的分布特性。
3. **理解数据集的难度**：
    
    - 朴素分类器的性能提供了有关数据集难度的信息。如果朴素分类器的准确性很高，说明数据集中某些类别非常常见，分类任务可能比较简单。如果朴素分类器的准确性较低，说明数据集的类别分布相对均匀，分类任务可能更具挑战性。

如果模型的准确性是85%，而朴素分类器的准确性是80%，这表明模型在数据中学到了有用的模式，超过了简单的朴素分类器。如果模型的准确性只有78%，那么模型的效果可能不如朴素分类器，这意味着模型可能需要进一步改进。